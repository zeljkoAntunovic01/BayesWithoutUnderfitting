Task 2:
[X] 1. Play with null mask threshold 
[X] 2. Optimize the MSE sigma factor for the HEssian alculation: H = (1 / sigma) * I 
[X] 3. Steal the metrics from this paper https://arxiv.org/abs/2106.14806  (dont use the library, steal code instead)

Task 3:
[X] 1. Change the Hessian for the classification (read up on it, it will be idenitity + outer product predicted class probabilities (p @ p.T))
[X] 2. Implement metrics on the Sine curve first (non classification ones)
[X - not relevant] 3. Switch to alternating projections for Sine first and then others
[X] 4. 2D dataset to visualize the decision boundary
[X] 5. Implement metrics for the classification models

Task 4:
[X] 1. Make the 2D dataset less noisy
[X] 2. Switch to Alternating projections for the 2D dataset
[X] 3. Finish up the Q_PROJ and Q_LOSS with the naive appraoch by adding an identity matrix to the GGN so it can be decomposed via Eigenvalues
 
Task 5:
[X] 1. Optimize the projections code so that it is vectorized and does not instantiate big matrices (or any matrices)
[X] 2. Add a condition for convergence such as change in norm < 10e-4 or something
[X] 3. Run on MNIST

Task 6:
[] 1. HPC run for MNIST and CIFAR10
[] 2. Alpha estimation via the formula from the paper -> Trace is not easy so we need to figure out a fast way of calclating it
[] 3. Do Eigenvalue/Eigvecs calculation in parallel for many batches at once, during the precomptuing of them (like JAX does)
[] 4. AFter hPC run see how long it took and if we need to optimize it more